{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "# 加载 LLaMA 分词器\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/data/LLM_models/llama-7b', token=\"your_token\")\n",
    "\n",
    "# 示例文本\n",
    "text = \"Hello, my name is Kimi and I am a large language model.\"\n",
    "\n",
    "# 使用分词器编码文本\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# 查看 input_ids 的长度\n",
    "print(\"Length of input_ids:\", inputs[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 2, ('cat',): 1, ('is',): 1, ('on',): 1, ('mat',): 1})\n",
      "Counter({('the',): 1, ('cat',): 1, ('s',): 1})\n",
      "2\n",
      "0.3333333333333333\n",
      "Counter({('the', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1})\n",
      "Counter({('the', 'cat'): 1, ('cat', 's'): 1})\n",
      "1\n",
      "0.2\n",
      "Counter({('the', 'cat', 'is'): 1, ('cat', 'is', 'on'): 1, ('is', 'on', 'the'): 1, ('on', 'the', 'mat'): 1})\n",
      "Counter({('the', 'cat', 's'): 1})\n",
      "0\n",
      "0.0\n",
      "Counter({('the', 'cat', 'is', 'on'): 1, ('cat', 'is', 'on', 'the'): 1, ('is', 'on', 'the', 'mat'): 1})\n",
      "Counter()\n",
      "0\n",
      "0.0\n",
      "[0.3333333333333333, 0.2, 0.0, 0.0]\n",
      "BLEU score: 0.2582\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngram_precision(candidate, reference, n):\n",
    "    # 提取候选字符串和参考字符串的n-grams\n",
    "    candidate_ngrams = Counter([counter for counter in zip(*[candidate[i:] for i in range(n)])])\n",
    "    reference_ngrams = Counter([counter for counter in zip(*[reference[i:] for i in range(n)])])\n",
    "    clipped_count = sum(min(candidate_ngrams[gram], reference_ngrams[gram]) for gram in candidate_ngrams)\n",
    "    all_count = sum(candidate_ngrams[gram] for gram in candidate_ngrams)\n",
    "    precision = clipped_count / all_count if candidate_ngrams else 0\n",
    "    return precision\n",
    "\n",
    "def brevity_penalty(candidate, reference):\n",
    "    if len(candidate) > len(reference):\n",
    "        return 1\n",
    "    ratio = len(candidate) / len(reference) if len(reference) > 0 else 0\n",
    "    return math.exp(1 - ratio) if ratio < 1 else 1\n",
    "\n",
    "def sentence_bleu(candidate, reference, max_n=4):\n",
    "    # 计算BLEU分数\n",
    "    p_ns = [ngram_precision(candidate, reference, n) for n in range(1, max_n + 1) ]\n",
    "    p_ns = [p for p in p_ns if p > 0]  # 移除0值\n",
    "    if not p_ns:\n",
    "        return 0  # 如果没有匹配的n-grams，则BLEU分数为0\n",
    "\n",
    "    geo_mean = math.exp(math.fsum(math.log(p) for p in p_ns) / len(p_ns))\n",
    "    bp = brevity_penalty(candidate, reference) \n",
    "    return bp * geo_mean\n",
    "\n",
    "# 示例使用\n",
    "candidate = \"the cat is on the mat\"\n",
    "references = \"the cat s\"\n",
    "\n",
    "score = sentence_bleu(candidate.split(), references.split())\n",
    "print(f\"BLEU score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
